<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding</h1>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><a href="#" target="_blank"><b>Tao Lin</b></a><sup>*</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Gen Li</b></a><sup>*</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yilei Zhong</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yanwen Zou</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Bo Zhao</b></a><sup>†</sup></span>
        </div>


                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <sup>1</sup>School of AI, Shanghai Jiao Tong University, 
                    <sup>2</sup>EvoMind Tech, 
                    <sup>3</sup>IAAR-Shanghai<br>
                  </span>
                  <span class="eql-cntrb">
                    <small><br><sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Author</small>
                  </span>
                </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.arxiv.org/pdf/2507.00416" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MINT-SJTU/Evo-VLA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2507.00416" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale text pretraining. However, VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. 
To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or defective estimation.
In contrast, our work introduces a plug-and-play module that implicitly injects 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation models. 
We design five spatially challenging tasks that require precise spatial understanding ability to validate effectiveness of our method. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Single fixed image display -->
<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Architecture of Evo-0</h2>
    <img src="static/images/model.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>

<!-- Single fixed image display -->
<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Tasks setup</h2>
    <img src="static/images/task_setup_3.jpg" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Comparison Videos (10x)</h2>

    <!-- Group 1 -->
    <div class="columns is-vcentered">
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Baseline(Pi0)</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/pi0_task1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Ours</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/vggt_task1.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <!-- Group 2 -->
    <div class="columns is-vcentered">
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Baseline(Pi0)</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/pi0_task2.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Ours</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/vggt_task2.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <!-- Group 3 -->
    <div class="columns is-vcentered">
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Baseline(Pi0)</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/pi0_task3.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Ours</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/vggt_task3.mp4" type="video/mp4">
        </video>
      </div>
    </div>

  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{lin2025evo,
  title={Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding},
  author={Lin, Tao and Li, Gen and Zhong, Yilei and Zou, Yanwen and Zhao, Bo},
  journal={arXiv preprint arXiv:2507.00416},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
